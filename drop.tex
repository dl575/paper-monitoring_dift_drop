\section{Enabling Coverage/Overhead Trade-off}
\label{sec:drop}

Filtering is only able to reduce overheads up to a certain point since there will still remain necessary monitoring operations. In some
contexts, this overhead may still be considered to be too high, forcing
monitoring to be disabled. Instead, it would be useful if partial monitoring
can be done in order to provide partial protection with further reduced
overheads. In this
section, we discuss how we can selectively perform monitoring in order to
further reduce overheads.

% Overview of dropping architecture
\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/drop_overview.pdf}
    \vspace{-0.3in}
    \caption{Block diagram of architecture for monitoring with adjustable overheads.}
    \label{fig:drop.overview}
    \vspace{-0.1in}
  \end{center}
\end{figure}

The goal is to be able to perform as much monitoring as possible within a specified overhead budget.
The high-level approach is to keep track of monitoring overheads at run-time
and drop monitoring operations when the overhead exceed the specified budget.
A block diagram of the architecture we will present in this section is shown in
Figure~\ref{fig:drop.overview}.  In order to limit the overheads of
monitoring, we need to first be able to track the gap between the specified
overhead budget and the monitoring overhead seen at run-time. We refer to this
as slack and discuss how we measure it in Section~\ref{sec:drop.slack}. 
Dropping monitoring operations can cause false positives which we prevent by
extending the dataflow engine to track and filter out dropped monitoring flows (see
Section~\ref{sec:drop.false_positives}).
Given that we know how much slack we have for monitoring and can drop
monitoring operations without false positives, we must decide when and which
monitoring operations to drop.  There exists some interesting trade-offs in the
policy for this dropping decision which we discuss in
Section~\ref{sec:drop.policies}.

\subsection{Measuring Overheads}
\label{sec:drop.slack}

% Slack
\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/slack.pdf}
    \vspace{-0.3in}
    \caption{Slack and its effect on monitoring over time.}
    \label{fig:drop.slack}
    \vspace{-0.1in}
  \end{center}
\end{figure}

% Slack tracking module
\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/stm.pdf}
    \vspace{-0.2in}
    \caption{Slack tracking and drop decision hardware.}
    \label{fig:drop.stm}
    \vspace{-0.1in}
  \end{center}
\end{figure}

In order to decide whether a monitoring event should be dropped or not, we need
a way to keep track of overheads. In this section, we present a specific method
for measuring execution time overheads. However, we note that other methods
could be used in order to target budgets on IPC, power, energy, etc.  
% For example, one way to measure execution time overheads would be to insert
% checkpoints in the program. By recording the execution time of these
% checkpoints without monitoring, the overheads incurred while running with
% monitoring could be determined. Although this would give an accurate
% measurement of overheads, it would require modifying the program binary. In
% addition, the overhead budget for monitoring would only be updated at
% checkpoints.

% Instead, we develop a model to estimate the execution time overhead budget in a
% gradual manner and without the need to modify the main program. Specifically,
We specify the overhead budget as a percentage of the main program's execution
cycles without monitoring. We define \emph{slack} as the number of cycles of
monitoring overhead that can be incurred while staying within the budget
target. Slack is essentially the difference between the actual overheads seen
and the budget specified. Slack is generated as the main program runs and consumed as monitoring overheads occur.  For
example, if no monitoring overheads occur during 1000 cycles of the main
program's execution and the designer sets a 20\% overhead target, then the
slack that is built up during this period is 200 cycles. If the main core is
then stalled for 50 cycles due to monitoring, then the remaining slack is 150
cycles. 
In addition to this accumulated slack, a small amount of initial slack
can be given in order for monitoring to be performed at the start of a program.
Figure~\ref{fig:drop.slack} shows an example of how slack can change over time.
If the slack falls below zero (i.e., the overhead budget is exceeded), then
monitoring events should be dropped.

Figure~\ref{fig:drop.stm} shows a hardware slack tracking module for
keeping track of slack. The slack tracking module uses a counter that increments on every $k$-th
cycle of the main core. This $k$ can be calculated by taking the reciprocal of
the target budget. For example, if the target budget is 20\%, then the counter
increments on every 5th clock cycle. The value of this counter is the
accumulated slack. Whenever the main core is stalled due to the monitor, slack
is decremented.

% In addition to keeping track of overheads, a policy must be developed to
% determine when monitoring event should be dropped. Complex policies involving
% multiple conditions and safety factors could be used to finely tune
% the trade off between monitoring and performance overhead. Similarly, introducing
% randomness into the policy could allow for better sampling in a cooperative
% debugging application of monitoring. Here, we present one simple
% dropping policy that is based on the accumulated slack.
% When the monitor is initialized, the monitor will load
% its needed slack in order to perform a monitoring operation into a register. When
% there is a monitoring event in the FIFO to be processed, a hardware comparator
% checks whether the accumulated slack is greater than or equal to the necessary
% slack for full monitoring. If enough slack exists, the monitor is signaled to
% perform the monitoring operation. Otherwise, the monitoring event is dropped.

\subsection{Preventing False Positives}
\label{sec:drop.false_positives}

Dropping a monitoring operation implies that some functionality of the monitor
has been lost. This may cause either false negatives, where an error that
occurs in the main program's execution is not detected, or false positives,
where the monitor incorrectly believes an error has occurred. 
% For example, a
% false positive can occur for UMC if a store monitoring event is dropped. This
% causes the store memory location to not be marked as initialized. A subsequent
% load for the memory location will incorrectly cause an error to be raised.  
For example, a false positive can occur for array bounds check if an event is
dropped that handles copying a pointer. A subsequent array access using the new
pointer will incorrectly cause an error to be raised since the new pointer's
base and bounds will not be correctly set.
We
accept false negatives as the loss in coverage that we trade off in order to
limit overheads.  However, we must safely drop monitoring events in such a way
as to avoid false positives so that the system does not incorrectly raise an
error.

In analyzing various monitoring schemes, we found that monitoring operations
are primarily of two types: \emph{checks} and \emph{metadata updates}. Monitors
\emph{check} certain properties to ensure correct main program execution and
they \emph{update} metadata for bookkeeping. Skipping a check operation can
only cause false negatives and will never cause a false positive. Therefore, we
may simply skip a check operation. Skipping an update operation can cause false
negatives but may also cause false positives. Essentially, when an update
operation is skipped, we can no longer trust the corresponding metadata.  Thus,
by associating an invalid flag with each metadata we can mark metadata that is
not correctly updated and use this information to prevent false positives.

% % Flag propagation rules for BC
% \begin{table}[tb]
%   \begin{center}
%     \begin{small}
%     \input{tabs/propagation_rules}
%     \end{small}
%     \caption{Flag propagation rules for array bounds check. A dash indicates that no
%     propagation is done while a 1 indicates that the destination flag is set.}
%     \label{tab:drop.propagation_rules}
%     \vspace{-0.2in}
%   \end{center}
% \end{table}

We can track these invalid flags by extending the dataflow engine presented in
Section~\ref{sec:filter} to keep track of an additional bit for each flag. That
is, we use the dataflow engine to track 2-bit flags in order to keep track of
both clean/dirty information as well as valid/invalid information.
Similar to filtering for clean flags, we can also filter out
monitoring operations corresponding to invalid metadata which would be useless to perform.
However, it is important that we propagate the
invalidation flag in these circumstances so that the entire flow is marked as invalid. 
In the typical case, the propagation rule used is
that if any of the source operands is marked as invalid, then the destination
operand is marked as invalid. This is slightly different than the
policy for clean flags which propagates a clean flag only if both
sources are marked as clean. 
% Table~\ref{tab:drop.propagation_rules} shows the
% propagation rules that are used for array bounds check. 
We note that
the propagation rules can be customized depending on the monitor by
reconfiguring the filter decision table.

\subsection{Dropping Decision}
\label{sec:drop.policies}

% Varying slack's impact on coverage for UMC
\begin{figure}
  \begin{center}
    \includegraphics[width=\columnwidth]{figs/dataflow_graph.pdf}
    \vspace{-0.3in}
    \caption{Example dependence graph for metadata. Square nodes represent
    events where checks are performed.} 
    \label{fig:drop.dataflow_graph}
    \vspace{-0.1in}
  \end{center}
\end{figure}

The simplest policy for when to drop monitoring events is to drop all monitoring events as
long as slack is less than or equal to zero.  However, this can result in
wasted work. For example, consider the metadata dependence graph shown in
Figure~\ref{fig:drop.dataflow_graph}. Here, an edge from node {\tt A} to node
{\tt B} represents that if event {\tt A} is dropped, then due to its
invalidated metadata, it will cause event {\tt B} to be also dropped. Square
nodes indicate events where monitoring checks are performed. In the
example, suppose that event {\tt E} is meant to perform a check operation but is dropped.
In this situation, the
monitoring operations that were done for events {\tt C} and {\tt D} were wasted
since their results were not used for any monitoring checks.

\begin{figure}
  \begin{center}
  \subfloat[all-event dropping]{
    \includegraphics[width=\columnwidth]{figs/all_drop.pdf}
    \label{fig:drop.all_drop}
  }
  \vspace{-0.1in}
  \\
  \subfloat[source-event dropping]{
    \includegraphics[width=\columnwidth]{figs/source_drop.pdf}
    \label{fig:drop.source_drop}
  }
  \vspace{-0.1in}
  \\
  \subfloat[min-event dropping]{
    \includegraphics[width=\columnwidth]{figs/min_drop.pdf}
    \label{fig:drop.min_drop}
  }
  \end{center}
  \vspace{-0.2in}
  \caption{Comparison of dropping policies. Blue (dark) nodes indicate where
  dropping decisions are made.}
  \label{fig:drop.policies}
\end{figure}

One method to eliminate this wasted work is to only make dropping decisions at
the root of these metadata flows. That is, we will decide to either monitor or
not monitor an entire metadata flow. We refer to this dropping decision policy
as \emph{source-event dropping} and we refer to the previous policy of 
dropping any event as \emph{all-event dropping}.
Figures~\ref{fig:drop.all_drop} and \ref{fig:drop.source_drop} shows a
comparison of where dropping decisions are made for these two policies. Source
dropping will
result in no wasted work. However, because of the coarser-grained decision, it
may be more difficult to closely match overheads. Source dropping is expected
to work well when there are a large number of independent metadata flows.

Another possible method is to make the dropping decisions somewhere in between
source-event dropping and all-event dropping. Looking at the graph from 
Figure~\ref{fig:drop.dataflow_graph}, what we would like to do is make a
decision at event {\tt C} whether to perform monitoring on that branch of the
metadata flow. If we choose to perform monitoring, then we want to perform all
monitoring on that flow. Instead, if we choose to drop event {\tt C}, then the
entire flow is dropped with no wasted work. Similarly, dropping event {\tt F}
drops the bottom flow with no wasted work and event {\tt A} can be dropped to
drop the entire shown metadata flow with no wasted work. If the program can be
analyzed or profiled to identify these instructions which, if dropped, lead to
no wasted work, then this information can be used at run-time to minimize the
amount of wasted work. We refer to this policy as \emph{min-event dropping}.
Figure~\ref{fig:drop.min_drop} shows the points were dropping decisions are
made for this policy.
Min-event dropping is a coarser-grained decision than all-event dropping, but
finer-grained that source dropping. Thus, its ability to match overheads is
expected to sit between these other two policies. Similarly, there is still a
chance of wasted work due to strange splitting and merging metadata flow graph
shapes, but typically we expect much less wasted work than all-event 
dropping. Thus, we expect the coverage achieved by min-event dropping for a
certain overhead to fall between source and all-event dropping. These three
dropping policies create a trade-off space between coverage achieved and
ability to match an overhead target which we evaluate in detail in
Section~\ref{sec:evaluation}.

% \subsection{Multiple-Run Coverage}
% Coverage can be considered for a single or average run instance. It can also be
% measured over multiple users/runs in order to capture total bug coverage of a
% program.
% 
% The system presented so far operates in a deterministic manner and so each run
% will perform the same monitoring checks. However, we can introduce randomness
% in order to achieve different dropping patterns over runs and achieve higher
% multiple-run coverage.
% 
% Due to the multiplicative effect of randomness, long dependence flows with
% multiple points where randomness is introduced leads to a small chance that the
% entire flow is monitored. This is a significant issue for last-minute dropping
% where all events could be dropped. Min-point dropping reduces this effect by
% reducing the points where dropping decisions are made while source-dropping
% eliminates this issue entirely.
% 
% \subsection{Multi-Core Implementation}
% 
% This can be extended to mulit-core by using multiple dataflow engines and a
% similar coherency protocol as that used for data caches.
