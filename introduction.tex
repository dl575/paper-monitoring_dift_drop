\section{Introduction}
\label{sec:intro}

% Parallel run-time useful
Run-time monitoring techniques have been shown to be useful for improving the
reliability, security, and debugging capabilities of computer systems. For
example, Hardbound is a hardware-assisted technique to detect out-of-bound
memory accesses, which can cause undesired behavior or create a security
vulnerability if uncaught \cite{hardbound-asplos08}.  Intel has
recently announced plans to support buffer overflow protection similar to
Hardbound in future architectures \cite{intel-mpx}. Similarly, run-time
monitoring can enable many other new security, reliability, and debugging
capabilities such as fine-grained memory protection \cite{mondrian-asplos02},
information flow tracking \cite{dift-asplos04, testudo-micro08}, hardware error
detection \cite{argus-micro07}, data-race detection \cite{radish-isca12,
cord-hpca06}, etc. 

% Programmable hardware monitors
These run-time monitors introduce performance, power, and energy overheads.
Implementing run-time monitoring in software using binary instrumentation or
other similar methods has been shown to introduce high overheads. For example,
dynamic information flow tracking (DIFT) implemented in software suffers a 3.6x
slowdown \cite{lift-micro06}. Implementing monitors in hardware greatly decreases
the performance impact by performing monitoring in parallel to a program's
execution. A dedicated hardware implementation of DIFT reduces performance
overheads to just 1.1\% \cite{dift-asplos04}. However, fixed hardware loses
the programmability of software, and cannot be updated or changed.

% Filtering
Recent studies have shown that filtering out monitoring checks on clean or null
metadata can be used to reduce the overheads of monitoring \cite{fade-hpca14}. For example, for
an array bounds check, checking non-pointer accesses is unnecessary. In this
paper, we present an architecture that combines a hardware-based dataflow
tracking engine with a software-based monitor in order to filter out monitoring operations 
for null metadata. By keeping a separate set of flags
to track null metadata, we are able to handle the common case of null
metadata without the need to read the full metadata values. By propagating these flags, we
are also able to handle propagation of null metadata efficiently in hardware.
This filtering can be used to greatly reduce the overheads of monitoring.

% Adjustable overhead
There is a limit to how effective filtering is at reducing monitoring overheads. 
Traditionally, these overheads have been an all-or-nothing cost. If the
designer finds that the overheads exceed their desired budget, then monitoring
cannot be enabled.  
Thus, we propose to perform partial monitoring to enable further reduction of overheads.
 This is achieved by dynamically dropping
monitoring operations when overheads exceed a specified overhead budget. We prevent false
positives from partial monitoring by using the dataflow engine to track these dropped
monitoring flows. We investigate different policies for deciding which events
to drop for partial monitoring and show that there is a trade-off between
closely matching the overhead budget and the monitoring coverage achieved.

Although full monitoring coverage is always desired, trading off coverage for
reduced overheads can be very useful. For example, this adjustable
monitoring enables a level of protection even for systems where the full
monitoring overheads are too high. This is especially true for energy- or
power-constrained systems or soft real-time systems where the monitoring
overhead should not exceed energy/power limits or real-time deadlines.
Additionally, adjustable monitoring can be used for debugging purposes
especially in the context of sampling-based or cooperative debugging techniques
which expect low coverage per run but use a large number of runs and users to
collect debugging information~\cite{liblit-pldi05, chilimbi-asplos04,
greathouse-cgo11}. 

The main contributions of this paper are summarized as follows.
\begin{itemize}
  \item We propose two optimizations for reducing the overheads of monitoring: filtering null metadata and performing partial monitoring.
  \item We present a hardware architecture using a dataflow tracking engine to efficiently enable filtering and partial monitoring.
  \item We investigate trade-off decisions for partial monitoring on the proposed architecture.
\end{itemize}

% %Invalidation in order to prevent false positives.
% In order to adjust monitoring overheads, a system must be able to drop a
% portion of the monitoring operations. Unfortunately, however, simply skipping
% monitoring operations can lead to false positives (false alarms) due to stale
% metadata.  One possible solution is to rewrite individual monitors to create
% ``drop-enabled'' versions.  However, such monitor-specific customizations are
% difficult and time consuming.  Instead, after analyzing a range of monitoring
% techniques, we found that by adding a 1-bit ``invalid'' flag to the bookkeeping
% metadata managed by the monitor, we were able to create a general mechanism to
% handle false positives across a broad range of monitoring techniques.  The
% result is that we have designed a hardware module that sits between a
% processing core and a programmable monitor, and allows adjustable overheads.
% This hardware module can be applied to a range of monitoring techniques and
% requires no modifications to the monitor.

% Evaluation
In order to evaluate our approach, we applied it to 3 different monitoring
techniques. These monitoring techniques vary in what events they monitor, the
size and semantics of their metadata, and the operations performed on metadata.
Our experiments show that with 10\% overheads, we can still provide 49-90\% of
coverage on average depending on the monitoring technique. By increasing the
overhead budget, the coverage rate can be increased. Our architecture is able
to closely meet the specified budgets. For all but one benchmark, the overheads
seen were within 2\% of the target overhead. Our results show that X policy is
able to achieve very close overhead matching (within X\% on average) with an
average coverage of X\%. On the other hand, by using Y policy, the overhead
matching is not as high (within Y\% on average) but average coverage increases
to (Y\%).

% Section summary
This paper is organized as follows. Section~\ref{sec:monitoring} presents the
parallel run-time monitoring model that this paper assumes.
Section~\ref{sec:optimizations} presents the high-level approach of how we aim
to reduce overheads through and Section~\ref{sec:arch} describes our
hardware architecture that supports this. Section~\ref{sec:policies}
investigates the design space for dropping decisions for partial monitoring.
Section~\ref{sec:evaluation} presents our evaluation methodology and
results. Finally, we discuss related work in Section~\ref{sec:related} and
conclude in Section~\ref{sec:conclusion}.

