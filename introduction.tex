\section{Introduction}
\label{sec:intro}

% Parallel run-time useful
Run-time monitoring techniques have been shown to be useful for improving the
reliability, security, and debugging capabilities of computer systems. For
example, Hardbound is a hardware-assisted technique to detect out-of-bound
memory accesses, which can cause undesired behavior or create a security
vulnerability if uncaught \cite{hardbound-asplos08}.  Intel has
recently announced plans to support buffer overflow protection similar to
Hardbound in future architectures \cite{intel-mpx}. Similarly, run-time
monitoring can enable many other new security, reliability, and debugging
capabilities such as fine-grained memory protection \cite{mondrian-asplos02},
information flow tracking \cite{dift-asplos04, testudo-micro08}, hardware error
detection \cite{argus-micro07}, data-race detection \cite{radish-isca12,
cord-hpca06}, etc. 

% Programmable hardware monitors
These run-time monitors introduce performance, power, and energy overheads.
% Implementing run-time monitoring in software using binary instrumentation or
% other similar methods has been shown to introduce high overheads. For example,
% dynamic information flow tracking (DIFT) implemented in software suffers a 3.6x
% slowdown \cite{lift-micro06}. 
Implementing monitors in dedicated hardware has been shown to have small
performance impacts. For example, a dedicated hardware implementation of
dynamic information flow tracking (DIFT) shows performance
overheads of only 1.1\% \cite{dift-asplos04}. However, fixed hardware loses
the programmability of software, and cannot be updated or changed. It 
requires for monitors to be chosen and implemented at hardware design time.
Recent studies \cite{lba-isca08, ruwase-spaa08, paralog-asplos10, oases-sigops09} have proposed using parallel cores in a
multi-core system to perform monitoring. This allows monitoring to be done
using parallel hardware while having the flexibility and programmability of
software. However, due to the multiple cycles it may take for software to
monitor an instruction, the overheads can easily increase execution time
several fold.
% Two optimizations
In this paper, we present a hardware architecture that uses dynamic dataflow
tracking in order to reduce the overheads of programmable parallel monitoring
by filtering out unnecessary monitoring operations and by enabling partial
monitoring.

% Filtering
Recent studies have shown that filtering out monitoring checks on clean or null
metadata can be used to reduce the overheads of monitoring \cite{fade-hpca14}.
For example, for
an array bounds check, checking non-pointer accesses is unnecessary. 
In this paper, we use a hardware-based dataflow tracking engine to track and
filter out these null metadata monitoring operations.
By using the dataflow engine to track a separate set of flags
indicating null metadata, we are able to handle the common case of null
metadata without the need to read the full metadata values.

There is a limit to how effective filtering is at reducing monitoring overheads
as it only eliminates unnecessary monitoring operations. The ability
to also skip relevant monitoring operations can be used to further reduce overheads.
Although this comes at the cost of reduced monitoring coverage (i.e.,
effectiveness), this partial monitoring can still be very useful.
For example, partial
monitoring can enable a level of protection even for systems where the full
monitoring overheads are too high. This is especially true for energy- or
power-constrained systems or soft real-time systems where the monitoring
overhead should not exceed energy/power limits or real-time deadlines.
Additionally, partial monitoring can be used for debugging purposes
especially in the context of sampling-based or cooperative debugging techniques
which expect low coverage per run but use a large number of runs and users to
collect debugging information~\cite{liblit-pldi05, chilimbi-asplos04,
greathouse-cgo11}. 

% % Adjustable overhead
In this paper, we show how our dataflow-guided architecture is also able to
enable partial monitoring. This is achieved by dynamically dropping monitoring
operations when overheads exceed a specified overhead budget. We prevent false
positives from partial monitoring by using the dataflow engine to track these
dropped monitoring flows. With this architecture, we investigate different
policies for deciding which events to drop for partial monitoring. These
different policies show a trade-off between closely matching the overhead
budget and the monitoring coverage achieved.

The main contributions of this paper are summarized as follows.
\begin{itemize}
  \item We propose two optimizations for reducing the overheads of monitoring: filtering null metadata and performing partial monitoring.
  \item We present a hardware architecture using a dataflow tracking engine to
  efficiently enable filtering and partial monitoring. This hardware
  architecture is designed to be generally applicable to a wide-range of run-time monitoring
  techniques.
  \item We investigate interesting trade-off decisions for partial monitoring on the proposed architecture.
\end{itemize}

% %Invalidation in order to prevent false positives.
% In order to adjust monitoring overheads, a system must be able to drop a
% portion of the monitoring operations. Unfortunately, however, simply skipping
% monitoring operations can lead to false positives (false alarms) due to stale
% metadata.  One possible solution is to rewrite individual monitors to create
% ``drop-enabled'' versions.  However, such monitor-specific customizations are
% difficult and time consuming.  Instead, after analyzing a range of monitoring
% techniques, we found that by adding a 1-bit ``invalid'' flag to the bookkeeping
% metadata managed by the monitor, we were able to create a general mechanism to
% handle false positives across a broad range of monitoring techniques.  The
% result is that we have designed a hardware module that sits between a
% processing core and a programmable monitor, and allows adjustable overheads.
% This hardware module can be applied to a range of monitoring techniques and
% requires no modifications to the monitor.

% Evaluation
In order to evaluate our approach, we applied it to 3 different monitoring
techniques. These monitoring techniques vary in what events they monitor, the
size and semantics of their metadata, and the operations performed on metadata.
Our experiments show that with 10\% overheads, we can still provide 49-90\% of
coverage on average depending on the monitoring technique. By increasing the
overhead budget, the coverage rate can be increased. Our architecture is able
to closely meet the specified budgets. For all but one benchmark, the overheads
seen were within 2\% of the target overhead. Our results show that X policy is
able to achieve very close overhead matching (within X\% on average) with an
average coverage of X\%. On the other hand, by using Y policy, the overhead
matching is not as high (within Y\% on average) but average coverage increases
to (Y\%).

% Section summary
This paper is organized as follows. Section~\ref{sec:monitoring} presents the
parallel run-time monitoring model that this paper assumes.
Section~\ref{sec:optimizations} presents the high-level approach of how we aim
to reduce overheads through and Section~\ref{sec:arch} describes our
hardware architecture that supports this. Section~\ref{sec:policies}
investigates the design space for dropping decisions for partial monitoring.
Section~\ref{sec:evaluation} presents our evaluation methodology and
results. Finally, we discuss related work in Section~\ref{sec:related} and
conclude in Section~\ref{sec:conclusion}.

