\section{Introduction}
\label{sec:intro}

% Parallel run-time useful
Run-time monitoring techniques have been shown to be useful for improving the
reliability, security, and debugging capabilities of computer systems. For
example, Hardbound is a hardware-assisted technique to detect out-of-bound
memory accesses \cite{hardbound-asplos08}.  Intel has
recently announced plans to support buffer overflow protection similar to
Hardbound in future architectures \cite{intel-mpx}. Similarly, run-time
monitoring can enable many other new security, reliability, and debugging
capabilities such as fine-grained memory protection \cite{mondrian-asplos02},
information flow tracking \cite{dift-asplos04, testudo-micro08}, hardware error
detection \cite{argus-micro07}, data-race detection \cite{radish-isca12,
cord-hpca06}, etc. 

% Programmable hardware monitors
% These run-time monitors introduce performance, power, and energy overheads.
% Implementing monitors in dedicated hardware has been shown to have small
% performance impacts. For example, a dedicated hardware implementation of
% dynamic information flow tracking (DIFT) shows performance
% overheads of only 1.1\% \cite{dift-asplos04}. However, fixed hardware loses
% the programmability of software, and cannot be updated or changed. It 
% requires for monitors to be chosen and implemented at hardware design time.
% Recent studies \cite{lba-isca08, ruwase-spaa08, paralog-asplos10, oases-sigops09} have proposed using parallel cores in a
% multi-core system to perform monitoring. This allows monitoring to be done
% using parallel hardware while having the flexibility and programmability of
% software. However, due to the multiple cycles it may take for software to
% monitor an instruction, the overheads can easily increase execution time
% several fold.
% % Two optimizations
% Since these overheads can be prohibitive for run-time use, we propose to enable
% partial monitoring in order to reduce the overheads.

Previous studies have explored various design points in terms of efficiency,
flexibility, and hardware costs for implementing run-time monitors. Custom
hardware designs that target a single or narrow-range of monitors have been
shown to have low performance overhead. On the other hand, flexible systems
that support a range of monitors have shown much higher overhead. In this
paper, we explore a new trade-off dimension for run-time monitor designs. We
propose to enable low overhead monitors without sacrificing flexibility by
using partial monitoring. In essence, this enables a trade-off between overhead and
monitoring coverage or accuracy.

Although partial monitoring comes at the cost of reduced monitoring coverage (i.e.,
effectiveness), this can still be useful in many scenarios.
For example, partial
monitoring can enable a level of protection even for systems where the full
monitoring overhead is too high. This is especially true for energy- or
power-constrained systems or soft real-time systems where the monitoring
overhead should not exceed energy/power limits or real-time deadlines.
Additionally, partial monitoring can be used for sampling-based or cooperative debugging techniques
which expect low coverage per run but use a large number of runs and users to
collect debugging information~\cite{liblit-pldi05, chilimbi-asplos04,
greathouse-cgo11}. 

% Adjustable overhead
Partial monitoring with adjustable overhead is achieved by dynamically
dropping monitoring operations when the overhead exceeds a specified overhead
budget. Although this results in reduced monitoring coverage (i.e., false
negatives), false positives can also occur due to out-of-date metadata information.
In order to prevent these
false positives, we need to track the dropped metadata flows.  We present a
hardware-based dataflow tracking engine that keeps track of these dropped
flows. With this architecture, we investigate different policies for deciding
which events to drop for partial monitoring. These different policies show a
trade-off between closely matching the overhead budget and increasing the monitoring
coverage.
In addition to enabling partial monitoring, we show how a simple
extension to our dataflow engine can enable null metadata filtering.
For example, for an array bounds check, checking non-pointer accesses is
unnecessary and can be filtered out using the dataflow engine.

The main contributions of this paper are summarized as follows:
\begin{itemize}
  \item We present a hardware architecture using a dataflow tracking engine to
  efficiently enable partial monitoring while preventing false positives. This
  hardware architecture is designed to be generally applicable to a wide-range
  of run-time monitoring techniques and monitor implementations.
  \item We investigate multiple policies on when and which operations to drop 
  for partial monitoring, and show the trade-offs in the design space.
  \item We extend the dataflow engine to enable filtering null metadata to
  reduce overhead.
\end{itemize}

% %Invalidation in order to prevent false positives.
% In order to adjust monitoring overheads, a system must be able to drop a
% portion of the monitoring operations. Unfortunately, however, simply skipping
% monitoring operations can lead to false positives (false alarms) due to stale
% metadata.  One possible solution is to rewrite individual monitors to create
% ``drop-enabled'' versions.  However, such monitor-specific customizations are
% difficult and time consuming.  Instead, after analyzing a range of monitoring
% techniques, we found that by adding a 1-bit ``invalid'' flag to the bookkeeping
% metadata managed by the monitor, we were able to create a general mechanism to
% handle false positives across a broad range of monitoring techniques.  The
% result is that we have designed a hardware module that sits between a
% processing core and a programmable monitor, and allows adjustable overheads.
% This hardware module can be applied to a range of monitoring techniques and
% requires no modifications to the monitor.

% Evaluation
In order to evaluate our approach, we applied it to five different monitoring
techniques. These monitoring techniques vary in what events they monitor, the
size and semantics of their metadata, and the operations performed on metadata.
% Our experiments show that filtering reduces the average overhead of these
% monitors from 7-18x down to 13-300\%.
% In addition, with partial monitoring, for the monitoring schemes which still
% see average overheads of 300\%, we show that with just 10\% overheads, we can still
% provide 32-82\% of coverage on average. By increasing the
% overhead budget, this coverage rate can be increased. 
These monitors show average slowdowns of 1.1-5.8x with the null metadata filtering when
implemented on a multi-core platform.
Our results show that partial monitoring can still achieve significant
coverage with reduced overhead. For example, for the monitors which showed over 2x
average overhead, slowdown could be cut down to 1.5x while still achieving a
coverage of 14-85\% on average.

% Section summary
This paper is organized as follows. Section~\ref{sec:monitoring} introduces the notion
of adjustable overhead through partial monitoring.
Section~\ref{sec:dropping} discusses the hardware architecture that enables 
partial monitoring using our dataflow tracking engine.
Section~\ref{sec:policies}
investigates the design space for dropping policies that determine when and which
monitoring operations to drop.
Section~\ref{sec:evaluation} presents our evaluation methodology and
results. Finally, we discuss related work in Section~\ref{sec:related} and
conclude in Section~\ref{sec:conclusion}.

