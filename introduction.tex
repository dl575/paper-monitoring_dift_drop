\section{Introduction}
\label{sec:intro}

% Parallel run-time useful
Run-time monitoring techniques have been shown to be useful for improving the
reliability, security, and debugging capabilities of computer systems. For
example, Hardbound is a hardware-assisted technique to detect out-of-bound
memory accesses, which can cause undesired behavior or create a security
vulnerability if uncaught \cite{hardbound-asplos08}.  Intel has
recently announced plans to support buffer overflow protection similar to
Hardbound in future architectures \cite{intel-mpx}. Similarly, run-time
monitoring can enable many other new security, reliability, and debugging
capabilities such as fine-grained memory protection \cite{mondrian-asplos02},
information flow tracking \cite{dift-asplos04, testudo-micro08}, hardware error
detection \cite{argus-micro07}, data-race detection \cite{radish-isca12,
cord-hpca06}, etc. 

% Programmable hardware monitors
These run-time monitors introduce performance, power, and energy overheads.
Implementing run-time monitoring in software using binary instrumentation or
other similar methods has been shown to introduce high overheads. For example,
dynamic information flow tracking (DIFT) implemented in software suffers a 3.6x
slowdown \cite{lift-micro06}. Implementing monitors in hardware greatly decreases
the performance impact by performing monitoring in parallel to a program's
execution. A dedicated hardware implementation of DIFT reduces performance
overheads to just 1.1\% \cite{dift-asplos04}. However, fixed hardware loses
the programmability of software, and cannot be updated or changed.

% Filtering
Recent studies have shown that filtering out monitoring checks on clean
metadata can be used to reduce the overheads of monitoring \cite{fade-hpca14}. For example, for
an array bounds check, checking non-pointer accesses is unnecessary. In this
paper, we present an architecture that combines a hardware-based dataflow
tracking engine with a software-based monitor in order to filter out both
propagation and checking of clean metadata. By keeping a separate set of flags
to track clean metadata, we are able to handle the common case of clean
metadata without the need to read full metadata. By propagating these flags, we
are also able to handle propagation of clean metadata efficiently in hardware.
Thus, we are able to have the flexibility of software-based monitors with
greatly reduced overheads.

% Adjustable overhead
With this filtering, we see overheads of XX\% - XX\% on average for
uninitialized memory check, array bounds check, and multi-bit dynamic
information flow tracking. 
Traditionally, these overheads have been an all-or-nothing cost. If the
designer finds that the overheads exceed their desired budget, then monitoring
cannot be enabled.  
Thus, we present an architecture that enables a trade-off between overheads and
the monitoring coverage achieved. This is achieved by dynamically dropping
monitoring operations when overheads are too high. We prevent false
positives by extending the dataflow engine to also track these dropped
monitoring flows. We investigate three different policies on when dropping decisions are
made that cover a trade-off space between the ability to match a target overhead
and the monitoring coverage achieved.

Although full monitoring coverage is always desired, trading off coverage for
reduced overheads can be very useful. For example, this adjustable
monitoring enables a level of protection even for systems where the full
monitoring overheads are too high. This is especially true for energy- or
power-constrained systems or soft real-time systems where the monitoring
overhead should not exceed energy/power limits or real-time deadlines.
Additionally, adjustable monitoring can be used for debugging purposes
especially in the context of sampling-based or cooperative debugging techniques
which expect low coverage per run but use a large number of runs and users to
collect debugging information~\cite{liblit-pldi05, chilimbi-asplos04,
greathouse-cgo11}. 

Thus, the main contributions of this paper are summarized as follows.
\begin{itemize}
  \item We use a dataflow tracking engine to efficiently filter out monitoring events in order to reduce overheads.
  \item We present an architecture that enables adjustable monitoring, creating a trade-off between monitoring coverage and overheads.
  \item We show how the dataflow tracking engine can be used to prevent false positives for adjustable monitoring.
  \item We explore the design space and trade-offs in dropping policies for performing adjustable monitoring.
\end{itemize}

% %Invalidation in order to prevent false positives.
% In order to adjust monitoring overheads, a system must be able to drop a
% portion of the monitoring operations. Unfortunately, however, simply skipping
% monitoring operations can lead to false positives (false alarms) due to stale
% metadata.  One possible solution is to rewrite individual monitors to create
% ``drop-enabled'' versions.  However, such monitor-specific customizations are
% difficult and time consuming.  Instead, after analyzing a range of monitoring
% techniques, we found that by adding a 1-bit ``invalid'' flag to the bookkeeping
% metadata managed by the monitor, we were able to create a general mechanism to
% handle false positives across a broad range of monitoring techniques.  The
% result is that we have designed a hardware module that sits between a
% processing core and a programmable monitor, and allows adjustable overheads.
% This hardware module can be applied to a range of monitoring techniques and
% requires no modifications to the monitor.

% Evaluation
In order to evaluate our approach, we applied it to 3 different monitoring
techniques. These monitoring techniques vary in what events they monitor, the
size and semantics of their metadata, and the operations performed on metadata.
Our experiments show that with 10\% overheads, we can still provide 49-90\% of
coverage on average depending on the monitoring technique. By increasing the
overhead budget, the coverage rate can be increased. Our architecture is able
to closely meet the specified budgets. For all but one benchmark, the overheads
seen were within 2\% of the target overhead. Our results show that X policy is
able to achieve very close overhead matching (within X\% on average) with an
average coverage of X\%. On the other hand, by using Y policy, the overhead
matching is not as high (within Y\% on average) but average coverage increases
to (Y\%).

% Section summary
This paper is organized as follows. Section~\ref{sec:monitoring} presents the
parallel run-time monitoring model that this paper assumes.
Section~\ref{sec:optimizations} presents the high-level approach of how we aim
to reduce overheads through and Section~\ref{sec:arch} describes our
hardware architecture that supports this. Section~\ref{sec:policies}
investigates the design space for dropping decisions for partial monitoring.
Section~\ref{sec:evaluation} presents our evaluation methodology and
results. Finally, we discuss related work in Section~\ref{sec:related} and
conclude in Section~\ref{sec:conclusion}.

