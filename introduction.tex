\section{Introduction}
\label{sec:intro}

% Parallel run-time useful
Run-time monitoring techniques have been shown to be useful for improving the
reliability, security, and debugging capabilities of computer systems. For
example, Hardbound is a hardware-assisted technique to detect out-of-bound
memory accesses, which can cause undesired behavior or create a security
vulnerability if uncaught \cite{hardbound-asplos08}.  Intel has
recently announced plans to support buffer overflow protection similar to
Hardbound in future architectures \cite{intel-mpx}. Similarly, run-time
monitoring can enable many other new security, reliability, and debugging
capabilities such as fine-grained memory protection \cite{mondrian-asplos02},
information flow tracking \cite{dift-asplos04, testudo-micro08}, hardware error
detection \cite{argus-micro07}, data-race detection \cite{radish-isca12,
cord-hpca06}, etc. 

% Programmable hardware monitors
% These run-time monitors introduce performance, power, and energy overheads.
% Implementing monitors in dedicated hardware has been shown to have small
% performance impacts. For example, a dedicated hardware implementation of
% dynamic information flow tracking (DIFT) shows performance
% overheads of only 1.1\% \cite{dift-asplos04}. However, fixed hardware loses
% the programmability of software, and cannot be updated or changed. It 
% requires for monitors to be chosen and implemented at hardware design time.
% Recent studies \cite{lba-isca08, ruwase-spaa08, paralog-asplos10, oases-sigops09} have proposed using parallel cores in a
% multi-core system to perform monitoring. This allows monitoring to be done
% using parallel hardware while having the flexibility and programmability of
% software. However, due to the multiple cycles it may take for software to
% monitor an instruction, the overheads can easily increase execution time
% several fold.
% % Two optimizations
% Since these overheads can be prohibitive for run-time use, we propose to enable
% partial monitoring in order to reduce the overheads.

Previous studies have explored various design points in terms of efficiency,
flexibility, and hardware costs for implementing run-time monitors. Custom
hardware designs that target a single or narrow-range of monitors have been
shown to have low performance overheads. On the other hand, flexible systems
that support a range of monitors have shown much higher overheads. In this
paper, we explore a new trade-off dimension for run-time monitor designs. We
propose to enable low overhead monitors without sacrificing flexibility by
using partial monitoring. In essence, this enables a trade-off between overheads and
monitoring coverage or accuracy.

Although partial monitoring comes at the cost of reduced monitoring coverage (i.e.,
effectiveness), this can still be useful in many scenarios.
For example, partial
monitoring can enable a level of protection even for systems where the full
monitoring overheads are too high. This is especially true for energy- or
power-constrained systems or soft real-time systems where the monitoring
overhead should not exceed energy/power limits or real-time deadlines.
Additionally, partial monitoring can be used for debugging purposes
especially in the context of sampling-based or cooperative debugging techniques
which expect low coverage per run but use a large number of runs and users to
collect debugging information~\cite{liblit-pldi05, chilimbi-asplos04,
greathouse-cgo11}. 

% Adjustable overhead
Partial monitoring with adjustable overheads is achieved by dynamically
dropping monitoring operations when overheads exceed a specified overhead
budget. In order to prevent dropped monitoring operations from leading to a
false positive, we need to track these dropped flows.  We present a
hardware-based dataflow tracking engine that keeps track of these dropped
flows. With this architecture, we investigate different policies for deciding
which events to drop for partial monitoring. These different policies show a
trade-off between closely matching the overhead budget and the monitoring
coverage achieved.

% Filtering
Recent studies have shown that filtering out monitoring operations on clean or null
metadata can be used to reduce the overheads of monitoring \cite{fade-hpca14}.
For example, for an array bounds check, checking non-pointer accesses is
unnecessary. In addition to enabling partial monitoring, we show how a simple
extension to our dataflow engine enables this null metadata filtering.

The main contributions of this paper are summarized as follows:
\begin{itemize}
  \item We present a hardware architecture using a dataflow tracking engine to
  efficiently enable partial monitoring while preventing false positives. This
  hardware architecture is designed to be generally applicable to a wide-range
  of run-time monitoring techniques.
  \item We investigate multiple policies on when and which operations to drop 
  for partial monitoring, and show the trade-offs in the design space.
  \item We extend the dataflow engine to enable filtering null metadata to
  reduce overheads.
\end{itemize}

% %Invalidation in order to prevent false positives.
% In order to adjust monitoring overheads, a system must be able to drop a
% portion of the monitoring operations. Unfortunately, however, simply skipping
% monitoring operations can lead to false positives (false alarms) due to stale
% metadata.  One possible solution is to rewrite individual monitors to create
% ``drop-enabled'' versions.  However, such monitor-specific customizations are
% difficult and time consuming.  Instead, after analyzing a range of monitoring
% techniques, we found that by adding a 1-bit ``invalid'' flag to the bookkeeping
% metadata managed by the monitor, we were able to create a general mechanism to
% handle false positives across a broad range of monitoring techniques.  The
% result is that we have designed a hardware module that sits between a
% processing core and a programmable monitor, and allows adjustable overheads.
% This hardware module can be applied to a range of monitoring techniques and
% requires no modifications to the monitor.

% Evaluation
In order to evaluate our approach, we applied it to 4 different monitoring
techniques. These monitoring techniques vary in what events they monitor, the
size and semantics of their metadata, and the operations performed on metadata.
% Our experiments show that filtering reduces the average overhead of these
% monitors from 7-18x down to 13-300\%.
% In addition, with partial monitoring, for the monitoring schemes which still
% see average overheads of 300\%, we show that with just 10\% overheads, we can still
% provide 32-82\% of coverage on average. By increasing the
% overhead budget, this coverage rate can be increased. 
These monitors show slowdowns of 1.1-4.3x with the null metadata filtering when
implemented on a multi-core platform.
%4-16x without null metadata filtering
%and 1.1-4.3x after null metadata filtering.  
Our results show that the partial monitoring can still achieve coverage of 43-82\%
on average at the reduced slowdown of 1.5x. 
The results also show that overhead and coverage can be varied to create a trade-off.

% Section summary
This paper is organized as follows. Section~\ref{sec:monitoring} introduces the notion
of adjustable run-time monitoring with partial monitoring.
Section~\ref{sec:dropping} discusses the hardware architecture that enables 
partial monitoring using our dataflow tracking engine.
Section~\ref{sec:policies}
investigates the design space for dropping policies that determine when and which
monitoring operations to drop.
Section~\ref{sec:evaluation} presents our evaluation methodology and
results. Finally, we discuss related work in Section~\ref{sec:related} and
conclude in Section~\ref{sec:conclusion}.

