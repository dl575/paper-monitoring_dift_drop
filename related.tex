\section{Related Work}
\label{sec:related}

% Monitoring
% Many monitoring schemes have been developed for various security, reliability,
% debugging, and other capabilities. There exist monitoring schemes for uninitialized
% memory check \cite{mondrian-asplos02}, array bounds check
% \cite{hardbound-asplos08, clause-ase07}, and dynamic information flow
% tracking \cite{dift-asplos04, raksha-isca07, loki-osdi08}.
% Memtracker \cite{memtracker-hpca07} uses monitoring to check for various
% memory-related bugs and errors.  Control flow verification techniques perform
% monitoring to check that program execution follows expected control flows
% \cite{schuette-comp87, impres-dac06,
% kayaalp-isca12}.  Our monitoring architecture is designed to
% be generally applicable to any of these instruction-grained monitoring schemes.

% In particular, our proposed architecture may be especially useful for some of
% these more complex schemes which also have higher overheads.  Similarly,
% although many of these schemes showed low overheads in custom hardware, our
% architecture can be used to reduce the overheads when applying these schemes on
% a multi-core platform.
 
% % Filtering
% FADE \cite{fade-hpca14} presents a general filtering engine in order to reduce
% the overheads of monitoring. It filters out redundant updates and null checks
% by reading metadata values. 
% By extending our dataflow engine for partial monitoring, we are able to also
% filter out null checks as well as null metadata propagations, but we do not
% handle filtering redundant updates.
% In contrast to FADE, our dataflow engine only
% requires reading and writing 2-bit flags rather than the entire metadata. 

% Previous work on sampling for monitoring
\begin{table}[t]
  \begin{center}
    \vspace{-0.0in}
    \begin{footnotesize}
    \input{tabs/previous_sampling}
    \end{footnotesize}
    \caption{Previous work on partial run-time monitoring.}
    \vspace{-0.2in}
    \label{tab:monitoring.previous_sampling}
  \end{center}
\end{table}

There exists a number of previous projects that have looked into performing
partial monitoring in order to reduce the performance overheads. These
platforms differ from ours in a number of ways including how monitoring is
implemented and the monitoring techniques targeted. However, we note three main
properties that differentiates our work:
\begin{enumerate}
  \item \textbf{Generality:} Our architecture applies to a variety of monitoring techniques.
  \item \textbf{Adjustable Overhead:} Our architecture allows an overhead to be
  targeted. Other work performs sampling to reduce overheads but does not try
  to bound overheads, which we have shown can vary greatly.
  \item \textbf{Prevent False Positives:} We prevent a mechanism to prevent
  false positives. Previous work either has false positives or targets monitoring
  techniques which degrade gracefully with sampling rather than exhibit false
  positives.
\end{enumerate}
Our work is the first that we know of to present a hardware platform for partial
monitoring that is general, allows an overhead target to be specified, and
explicitly prevents false positives. In contrast, almost all previous work on
partial monitoring achieves only some, but not all, of these properties.
Table~\ref{tab:monitoring.previous_sampling} summarizes these differences. 

For example, there exists previous work for using statistical sampling to
reduce the performance overheads of various debugging techniques. These include
sampling for bug isolation \cite{liblit-pldi05}, memory leak detection
\cite{chilimbi-asplos04}, race detection \cite{literace-pldi09, pacer-pldi10},
and information flow tracking \cite{testudo-micro08, greathouse-cgo11}. These
techniques modify the monitoring to support statistical sampling and so are not
generalizable. For those that prevent false positives, this is also done with
monitor-specific modifications. Finally, with the exception of the work by
Greathouse et al. \cite{greathouse-cgo11}, they do not allow an overhead target
to be specified.

There also exists several projects that have looked into more general partial
monitoring platforms. Arnold and Ryder \cite{arnold-pldi01} presented a general
platform for sampling of instrumented code, but do not allow an overhead target
to be specified and do not prevent false positives. Huang et al.
\cite{huang-sttt12} proposed a general framework for controlling the overheads
of software-based monitoring. However, they also do not explicitly address
false positives and only target monitors which degrade gracefully when
performed partially. Lo et al. \cite{lo-rtas14} designed a hardware
architecture for performing monitoring on hard real-time systems that also
prevents false positives. Their architecture is designed to meet real-time
deadlines rather than overhead targets. In addition, in order to give strong
guarantees, the coverage achieved is lower than our system. Finally, QVM
\cite{qvm-oopsla08} proposes a modification to the Java Virtual Machine (JVM) to
support monitoring with adjustable overheads. QVM does prevents false positives
but does so be enabling or disabling monitoring on a per-object basis. This is
similar to the idea of performing source-only dropping which our results show
can lead to overshooting the overhead target depending on the monitoring
technique. In addition, QVM is limited monitoring for code run on a JVM while
our framework works for any binary.

% In contrast, our work is the first that we know of to present a general hardware platform that enables
% partial monitoring. The architecture is designed to be applicable to a wide range of monitoring
% techniques as well as monitor implementations (core, FPGA, etc.). By studying various potential
% uses of partial monitoring, including sampling for cooperative debugging and
% partial protection for low overheads, we also identified important trade-offs in dropping
% policy decisions to consider in order to best meet the requirements of the
% application.

% % Sampling
% Statistical sampling is a method that has been applied to various debugging
% techniques in order to reduce their performance overheads. For example, Liblit
% et al. sampled program runs of thousands of users in order to find bugs
% \cite{liblit-pldi05}. This idea has also been applied to detecting memory
% leaks~\cite{chilimbi-asplos04}, dataflow analysis~\cite{greathouse-cgo11}, and
% dynamic information flow tracking~\cite{testudo-micro08}.
% These sampling-based techniques all limit the debugging capabilities in order
% to provide lower performance overheads, which is one of the target applications of our architecture.
% % Limited Monitoring
% In terms of partial monitoring with adjustable overheads, the Quality Virtual Machine (QVM) is a
% modification of the Java Virtual Machine (JVM) that supports run-time
% monitoring with controllable overheads \cite{qvm-oopsla08}. Similarly, Huang et
% al. created a framework for controlling the overheads of software-based
% monitoring \cite{huang-sttt12}. 
% Lo et al. \cite{lo-rtas14} developed a hardware architecture to limit
% monitoring to meet deadlines for hard real-time systems.

% However, these approaches are not able to explicitly set an overhead budget, as
% our system is able to do. In addition, with the exception of Testudo that is
% designed for one specific monitor, these
% methods have not considered parallel monitoring. Our techniques for generally
% reducing the overheads of monitoring can also be used to enable these
% low-overhead, multi-user/run debugging techniques.
% \ED{clarify how our approach is different. Is it more general - supporting
% a large class of monitors? Is it different because it is for parallel monitoring?
% If so, what's the significance of the parallel monitors?}

% These projects are targeting a similar problem
% to the one we address in this paper. However, both projects are focused on
% software-based monitoring and thus their mechanisms modify the software
% monitoring framework in order to enable or disable monitoring. 
% Instead, our
% architecture provides a general hardware mechanism to control overheads of
% parallel monitoring architectures, largely transparent to the original monitor. 
% In addition, these works prevented
% false positives by enabling and disabling monitoring using only a source-dropping
% policy. 
%  Their architecture performs unrestricted 
% dropping in order to guarantee strict deadlines. 
% Our system allows either source-only dropping or unrestricted dropping to be performed.
% This allows the designer or user to choose the appropriate policy depending on
% the monitoring scheme which has not been previously explored.
% \ED{Is the use of source vs. unrestricted dropping the only major difference for
% this work? That sounds rather weak.}
