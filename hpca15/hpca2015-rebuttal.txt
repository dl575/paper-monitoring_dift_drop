Common Questions

-Review-D,E: How does this compare to FADE?
Our main contribution in this work is an architecture which enables partial
monitoring (i.e., trade-off between coverage and overheads). We are not the
first to introduce null metadata filtering as a method for reducing monitoring
overheads (e.g., FADE). However, we show that our hardware support for
partial monitoring is able to support null metadata filtering with minor
modifications. Thus, it is not necessary to implement the full FADE hardware in
order to add null metadata filtering to our architecture for partial
monitoring.

-Review-A,B,E,F: Evaluation of false negatives/error detection rate.
While we could not evaluate real errors/attacks due to difficulty in setting up
a large number of bugs for multiple monitoring schemes, we believe that the
percentage of checks provides a good initial estimate of detection probability
when errors/attacks are uniformly distributed across checks. It will be
interesting as future work to evaluate the system with real-world bugs to see
if this distribution holds or if there exists correlations between bug
locations and slack.

Review-A

-Comparison to Raksha and [Kannan DSN2009].
Raksha and the architecture proposed in [Kannan DSN2009] provide hardware
architectures for information flow tracking. These architectures show low
overheads because they are designed for a specific monitoring technique.
Although Raskha provides some programmability in the information flow tracking
policy, it does not support the wide range of monitors supported by our
architecture.

-Guarantee to detect violations at instruction boundaries?
We do not enforce a check to be done before the monitored instruction finishes.
However, previous work [Deng ICCD2011] described how precise exceptions can be 
supported on such decoupled architectures.


Review-B

-Using another core for monitoring?
We explored performing monitoring on another core and FPGA fabric. Dropping
operations were implemented using custom hardware in order to minimize their
performance impact. We explored performing the dropping operations in software,
but found that that design leads to high overheads even when all events are
dropped.

Review-C

-Provide any precise guarantees regarding dropped requests?
The system does not provide strict guarantees on the overhead, but can closely
match the target in practice. Because run-time overheads are difficult to
predict a priori, there is no precise guarantee on what is dropped.

-Is hardware support really necessary? 
We found that hardware support is necessary to closely match the overhead
target (source dropping in software is insufficient) and provide meaningful coverage 
with low overhead. High per-system coverage is also important when the goal
is to provide a partial protection for individual systems.

-[7] has similarities to your approach.
Our baseline architecture is similar to the one proposed by [7] and other
related work. However, our work is the first to show how partial monitoring can
be supported on these architectures using a dataflow engine. 

Review-D

-How does this compare to ad-hoc filtering?
Previous work uses filtering to reduce the overheads of monitoring. However,
the overheads seen can vary wildly (see Figure 9). Our architecture allows a
target overhead to be specified. FlexiTaint [Venkataramani HPCA2008] does not
mention a drop-on-overhead policy.

Review-E

-Measuring coverage in errors detected? False positives?
For errors detected, see Common Questions. Our architecture does not generate
any false positives.

Review-F

-Energy consumed by full and partial monitors.
In our baseline monitoring architecture, monitoring uses another core and 
has the power consumption comparable to that of the main core. On the other hand,
the power consumption of the dropping hardware is often less than 10% of
the main core power consumption. Therefore, our architecture certainly saves
energy when monitoring operations are dropped. However, when almost no monitoring
operation is dropped, we found that the dropping hardware incurs a small
energy overhead. We will add more explicit energy comparisons.

-Tracking invalid tags through different threads? Coherence for dataflow
 monitor?
Invalid tags for different threads would be tracked by the dataflow engine
through shared memory. A multi-processor implementation of the architecture
would require coherence for the metadata, similar to what has been studied
for LBA and Raksha.

-How are sources identified? How do you decide to drop the source?
Sources are identified by instruction type or explicit SW instructions. For
example, for uninitialized memory check, all store instructions are configured
to be marked as sources. In addition, custom instructions used for setting
metadata (e.g., setting array bounds metadata) are marked as sources. Dropping
sources can be done based on slack or using a probabilistic approach.

-Sampling-based software tools.
Sampling-based methods can reduce overheads but overheads can vary
across applications. Results typically show that average overheads are low but
can include outliers. This is not surprising as the overheads without sampling
can vary (see Figure 9). Our architecture allows an overhead target to be
specified.

