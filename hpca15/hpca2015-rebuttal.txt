Common Questions

-Review-D,E: How does this compare to FADE?
Our main contribution in this work is an architecture which enables partial
monitoring (i.e., trade-off between coverage and overheads). We are not the
first to introduce null metadata filtering as a method for reducing monitoring
overheads (e.g., FADE). However, we show that our hardware support for
partial monitoring is able to support null metadata filtering with minor
modifications. Thus, it is not necessary to implement the full FADE hardware in
order to add null metadata filtering to our architecture for partial
monitoring.

-Review-A,B,E,F: Evaluation of false negatives/error detection rate.
While we could not evaluate real errors/attacks due to difficulty in setting up
a large number of bugs and exploits and/or running real-world applications on
our simulation infrastructure, we believe that the percentage of checks
provides a good initial estimate of detection probability when errors/attacks
are uniformly distributed across checks. It will be interesting as future work
to evaluate the system with real-world bugs to see if this distribution holds
or if there exists correlations between bug locations and slack.

Review-A

-Comparison to Raksha and [Kannan DSN2009].
Raksha and the architecture proposed in [Kannan DSN2009] provide hardware
architectures for information flow tracking. These architectures show low
overheads because they are designed for a specific monitoring technique.
Although Raskha provides some programmability in the information flow tracking
policy, it does not support the wide range of monitors supported by our
architecture.

-Do you guarantee to detect violations at instruction boundaries?
We do not guarantee when violations are detected. However, previous work [Deng
ICCD2011] has described how precise exceptions can be supported on such an
architecture.

Review-B

-Using another core for monitoring tasks.
The monitoring task is performed by another core. Dropping operations were
implemented using custom hardware in order to minimize their performance
impact. We have explored performing these dropping operations in software.
However, this can lead to high overheads even when all events are dropped.

Review-C

-Can you provide any precise guarantees regarding dropped requests?
Dropping is decided based on run-time overheads. This is difficult to predict a
priori and so we are not able to give precise guarantees on what is dropped or
the coverage achieved.

-Is hardware support really necessary? Higher overhead/coverage ratio, but with
scaling out to many users, does it matter?
Table 1 shows that the gap in overhead between software and hardware solutions
for fine-grained monitoring can be an order of magnitude. Although a software
solution is easier to deploy, if the overhead/coverage ratio is too high, then
the number of users needed to detect an error or the minimum overhead to
achieve any coverage can be prohibitively high.

-[7] has similarities to your approach.
Our baseline architecture is similar to the one proposed by [7] and other
related work. However, our work is the first to show how partial monitoring can
be supported on these architectures using a dataflow engine. 

Review-D

-How much better does this do than ad-hoc filtering?
Previous work uses filtering to reduce the overheads of monitoring. However,
the overheads seen can vary wildly (see Figure 9). Our architecture allows a
target overhead to be specified. FlexiTaint [Venkataramani HPCA2008] does not
mention a drop-on-overhead policy.

Review-E

-Should we be measuring coverage in terms of errors detected, false positives
 generated?
For errors detected, see Common Questions. Our architecture does not generate
any false positives.

Review-F

-Energy consumed by full and partial monitors.
Dropping an event is less energy than performing full monitoring, but
the dataflow engine also adds energy overhead when not dropping. Thus, it is
not clear which uses more energy and it largely depends on the overhead
target and the number of events dropped. 

-Tracking invalid tags through different threads? Coherence for dataflow
 monitor?
Invalid tags for different threads would be tracked by the dataflow engine
through shared memory. A multi-processor implementation of the architecture
would require coherence for the caches of the dataflow engine and monitor,
similar to the coherence needed for the core caches.

-How are sources identified? How do you decide to drop the source?
Sources are identified by instruction type. For example, for uninitialized
memory check, all store instructions are configured to be marked as sources. In
addition, custom instructions used for setting metadata (e.g., setting array
bounds metadata) are always marked as sources. Dropping sources can be done
based on slack or using a probabilistic approach.

-Sample-based software tools.
Sampling-based methods typically can reduce overheads but overheads can vary
across applications. Results typically show that average overheads are low but
can include outliers. This is not surprising as the overheads without sampling
can vary (see Figure 9). Our architecture allows an overhead target to be
specified.

